---
title: "Factor analysis and other clustering methods"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

```{r echo = FALSE, results='hide', message=FALSE, comment=FALSE, warning=FALSE}
## load libraries
require(foreign)
require(tidyverse)
require(ReIns)
require("poweRlaw")
require(lavaan)
```

```{r, echo=FALSE}
dfPCL5 <- read.spss('/home/or/Documents/pcl5_vaData/pcl5_100k/PCL5_130K_FY19.sav', to.data.frame = TRUE)

dfPCL5 <- dfPCL5[,5:24]
dfPCL5 <- na.omit(dfPCL5)
```

```{r echo=FALSE}
dfPCL5_toBinarize <- dfPCL5
for (i in 1:20){
  # becuase under 10 its PCLN01 (02,03) need to add zero so run if
  if (i<10){
  nam <- paste("PCLN0", i, sep = "")  
  }
  else {
    nam <- paste("PCLN", i, sep = "")
  }
  pcl <- paste("PCL", i, sep = "")
  dfPCL5_toBinarize[pcl] <- dplyr::case_when(dfPCL5_toBinarize[nam]<=1 ~ 'NO', dfPCL5_toBinarize[nam]>=2 ~ 'YES')
  
}

# create specific dataframe
dfbigPCL5_Binary2 <- data_frame(dfPCL5_toBinarize[21:40])
countFreqBigPCL5 <- plyr::count(dfbigPCL5_Binary2)
```

## Factor analysis
- Here we try to present the effect of the power law distribution on factor analysis
- We also show it in other graphical manners 

```{r}
# Create new data frame
# join the two so we have frequency next to each individual
dataxCon <- dplyr::full_join(dfPCL5_toBinarize, countFreqBigPCL5, by = c("PCL1" = "PCL1", "PCL2" = "PCL2", "PCL3" = "PCL3", "PCL4" = "PCL4", "PCL5" = "PCL5", "PCL6" = "PCL6", "PCL7" = "PCL7", "PCL8" = "PCL8", "PCL9" = "PCL9", "PCL10" = "PCL10", "PCL11" = "PCL11", "PCL12" = "PCL12", 
 "PCL13" = "PCL13", "PCL14" = "PCL14", "PCL15" = "PCL15", "PCL16" = "PCL16", "PCL17" = "PCL17", "PCL18" = "PCL18", "PCL19" = "PCL19", "PCL20" = "PCL20"))

```

```{r}
dsm5Fac_Con <- '
      Int =~ PCLN01 + PCLN02 + PCLN03 + PCLN04 + PCLN05
      Av =~ PCLN06 + PCLN07
      An =~ PCLN08 + PCLN09 + PCLN10 + PCLN11 + PCLN12 + PCLN13 + PCLN14 
      Hyper =~ PCLN15 + PCLN16 + PCLN17 + PCLN18 + PCLN19 + PCLN20'

```

```{r}
iterateCFA <- function(data, n_itr, filter, sample_size=500) {
  
 # n_itr = 100 # set number of iterations
  results_con = matrix(nrow = n_itr, ncol = 6)#[1:n_itr, 6]
  # set filter (here we take all with freq lower than 10)
  if (filter=='>') {
    print('More than')
    dataN_Con <- filter(data, freq > 10)  
  } else {
    print ('Less than')
    dataN_Con <- filter(data, freq <= 10)  
  }
  
  for (i in 1:n_itr) {
    # filter and sample data
    dfSamp <- sample_n(dataN_Con, sample_size) 
    pcl5Model <- cfa(dsm5Fac_Con, data = dfSamp[,1:20], estimator = "WLSMV")
    results_con[i,] <- fitMeasures(pcl5Model, c("chisq","df","pvalue","srmr","cfi","rmsea"))
    
  }
  # create a dataframe from the matrix
  results <- data.frame(chisq = results_con[,1], df = results_con[,2],
                                               pvalue = results_con[,3], srmr = results_con[,4],
                                               cfi = results_con[,5], rmsea = results_con[,6])
  return(results)
}
```

```{r}
conBigger10 <- iterateCFA(dataxCon, n_itr = 100, filter = '>', sample_size = 300) # failing
MASS::truehist(conBigger10$cfi, xlab = "CFI of all that higher than 10")
# now lower than ten

conSmaller10 <- iterateCFA(dataxCon, n_itr = 100, filter = '<', sample_size = 300)
MASS::truehist(conSmaller10$cfi, xlab = "CFI of all that lower than 10")
summary(conSmaller10)
summary(conBigger10)
```


- The most frequent fits better.
- Lets test the effect of sample size on fitting (i.e. how much will be enough when we sampe most frequent and when we sample less).

### Sample size effects

```{r}
# most frequent
# vecotr from sample size of 50 to 1000 in steps of 50
size_n <- seq(100,1000, 50)
res <- c()
for (i in 1:length(size_n)){
  print('sample size {size_n}')
  conBigger10 <- iterateCFA(dataxCon, n_itr = 30, filter = '>', sample_size = size_n[i]) # failing
  res[i] <- median(conBigger10$rmsea)
}

d <- data.frame(res,size_n)
ggplot(data=d, aes(y=res, x=size_n)) + geom_jitter() + geom_smooth() + xlab("rmsea") + theme_minimal()
```

- Do the same with the lower tail of the distribution:
```{r}
res1 <- c()
for (i in 1:length(size_n)){
  print('sample size {size_n}')
  conSmaller10 <- iterateCFA(dataxCon, n_itr = 30, filter = '<', sample_size = size_n[i]) # failing
  res1[i] <- median(conSmaller10$rmsea)
}

d1 <- data.frame(res1,size_n)
ggplot(data=d1, aes(y=res1, x=size_n)) + geom_jitter() + geom_smooth() +  xlab("rmsea") + theme_minimal()
```
- Lastly, all 

```{r}
res2 <- c()
results_con <-  matrix(nrow = 30, ncol = 6)
for (i in 1:length(size_n)){
 # print('sample size {size_n}')
  for (n in 1:30){
    dfSamp <- sample_n(dfPCL5, size_n[i]) 
    pcl5Model <- cfa(dsm5Fac_Con, data = dfSamp[,1:20], estimator = "WLSMV")
    results_con[n,] <- fitMeasures(pcl5Model, c("chisq","df","pvalue","srmr","cfi","rmsea"))
    
  }
  
  res2[i] <- median(results_con[,6])
}

d2 <- data.frame(res2,size_n)
ggplot(data=d2, aes(y=res2, x=size_n)) + geom_jitter() + geom_smooth() +  xlab("rmsea") + theme_minimal()
```



