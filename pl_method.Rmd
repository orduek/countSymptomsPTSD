---
title: "Symptoms of PTSD follow a power law distribution"
author: "Or Duek & Tobias Spiller"
output:
  html_document:
    df_print: paged
---

# Power law (scale-free)
- This work assess fit of power-law to different data-sets in mental health
- We fit three different distributions for each set of data:
  1. Power-law
  2. log-normal
  3. exponential
  

```{r echo = FALSE, results='hide', message=FALSE, comment=FALSE, warning=FALSE}
## load libraries
require(foreign)
require(tidyverse)
require(ReIns)
require("poweRlaw")
require(lavaan)
require(FactoMineR)
require(factoextra)
# load specific functions
source('utils.r') # load functions
```

### First data-set 30k PTSD patients
- For each data set we:
  1. Load and clean data
  2. Binarize (?)
  3. Calculate profiles (combinations of symptoms) and their frequencies (how many patients per profile)
  
```{r}
dfPCL5 <- read.spss('/home/or/Documents/pcl5_vaData/pcl5_100k/PCL5_130K_FY19.sav', to.data.frame = TRUE)

dfPCL5 <- dfPCL5[,5:24]
dfPCL5 <- na.omit(dfPCL5)
```

```{r}
# check date
a = '01/01/1960'
dt = as.Date(as.character(a), format = '%d/%m/%Y') + dfPCL5$SurveyGivenDateTime

```


```{r echo=FALSE}
dfPCL5_toBinarize <- dfPCL5
for (i in 1:20){
  # becuase under 10 its PCLN01 (02,03) need to add zero so run if
  if (i<10){
  nam <- paste("PCLN0", i, sep = "")  
  }
  else {
    nam <- paste("PCLN", i, sep = "")
  }
  pcl <- paste("PCL", i, sep = "")
  dfPCL5_toBinarize[pcl] <- dplyr::case_when(dfPCL5_toBinarize[nam]<=1 ~ 'NO', dfPCL5_toBinarize[nam]>=2 ~ 'YES')
  
}

# create specific dataframe
dfbigPCL5_Binary2 <- tibble(dfPCL5_toBinarize[21:40])
countFreqBigPCL5 <- plyr::count(dfbigPCL5_Binary2)
```

### A total of 8174 combinations. 
- Describe frequencies

```{r}
## count how many with only one
sum(countFreqBigPCL5$freq==1)
# percentage
sum(countFreqBigPCL5$freq==1) / nrow(countFreqBigPCL5)

# less than 5
sum(countFreqBigPCL5$freq<=5)
# less than 50
sum(countFreqBigPCL5$freq<=50)
# more than 100
sum(countFreqBigPCL5$freq>100)
# count more than one less than ten
sum(countFreqBigPCL5$freq<10) - sum(countFreqBigPCL5$freq==1)
max(countFreqBigPCL5$freq)
max(countFreqBigPCL5$freq) / nrow(dfbigPCL5_Binary2)

# 
freq5 <- countFreqBigPCL5[order(countFreqBigPCL5$freq, decreasing = TRUE),]
freq5_top50 <- freq5[1:50,]
sum(freq5_top50$freq)

freq5_top10 <- freq5[1:10,]
sum(freq5_top10$freq)
```

- Plot the top 50 profiles

```{r, echo=FALSE}
freq5 <- countFreqBigPCL5[order(countFreqBigPCL5$freq, decreasing = TRUE),]
freq5_top50 <- freq5[1:50,]
x <- as.factor(1:nrow(freq5_top50))
ggplot(freq5_top50, aes(x=as.factor(x),y=freq)) + geom_bar(stat="identity") + xlab("Combination Of Symptoms") + theme_minimal()
```

### Test distributions
#### Power-law

```{r, echo=FALSE}
m_pl = displ$new(countFreqBigPCL5$freq)
est = estimate_xmin(m_pl) ## Estimate parameters
m_pl$setXmin(est) ## Update object
plot(m_pl)
lines(m_pl, col = 2) # draw fitted line (color=red)
```

* Use bootstrap to test significant level 

```{r, echo=FALSE}
bs_p = bootstrap_p(m_pl, no_of_sims = 2000, threads = 10)
plot(bs_p)
```

#### Log-Normal

```{r}
m_ln = dislnorm$new(countFreqBigPCL5$freq)
est_ln = estimate_xmin(m_ln) ## Estimate parameters
m_ln$setXmin(est_ln)
m <- plot(m_ln)
lines(m_ln, col=3)
```

```{r, echo=FALSE}
bs_ln = bootstrap(m_ln, no_of_sims = 1000, threads = 10)
plot(bs_ln)
```

#### Exponential

```{r}
m_exp = disexp$new(countFreqBigPCL5$freq)
est_exp = estimate_xmin(m_exp) ## Estimate parameters
m_exp$setXmin(est_exp)
```
```{r, echo=FALSE}
bs_exp = bootstrap_p(m_exp, no_of_sims = 1000, threads = 10)
plot(bs_exp)
```

### Compare distributions
- Lognormal and power-law

```{r}
m_ln$setXmin(m_pl$getXmin())
est2 = estimate_pars(m_ln)
m_ln$setPars(est2$pars)

comp = compare_distributions(m_pl, m_ln) # Test the two distributions
comp$p_two_sided #not sig: inconclusive
comp$p_one_sided #not sig: inconclusive
```
- results are inconclusive. 
- Compare to exponential

```{r}
m_exp$setXmin(m_pl$getXmin())
est3 = estimate_pars(m_exp)
m_exp$setPars(est3$pars)

comp2 = compare_distributions(m_pl, m_exp) # Test the two distributions
comp2$p_two_sided #not sig: inconclusive
comp2$p_one_sided #not sig: inconclusive
```
### Fitting plot of all three

```{r}
plot(m_pl)
lines(m_pl, col=2) # red
lines(m_ln, col = 3) # green
lines(m_exp, col = 4) # blue
```
It seems the power-law fitts a bit better then the log-normal and much better than the exponential


- Still inconclusive, but seems closer to significant differece.

## Factor analysis

```{r, echo=FALSE, include=FALSE}
# Create new data frame
# join the two so we have frequency next to each individual
dataxCon <- dplyr::full_join(dfPCL5_toBinarize, countFreqBigPCL5, by = c("PCL1" = "PCL1", "PCL2" = "PCL2", "PCL3" = "PCL3", "PCL4" = "PCL4", "PCL5" = "PCL5", "PCL6" = "PCL6", "PCL7" = "PCL7", "PCL8" = "PCL8", "PCL9" = "PCL9", "PCL10" = "PCL10", "PCL11" = "PCL11", "PCL12" = "PCL12", 
 "PCL13" = "PCL13", "PCL14" = "PCL14", "PCL15" = "PCL15", "PCL16" = "PCL16", "PCL17" = "PCL17", "PCL18" = "PCL18", "PCL19" = "PCL19", "PCL20" = "PCL20"))
```

- Define the model (according to DSM-5 clustering)

```{r}
dsm5Fac_Con <- '
      Int =~ PCLN01 + PCLN02 + PCLN03 + PCLN04 + PCLN05
      Av =~ PCLN06 + PCLN07
      An =~ PCLN08 + PCLN09 + PCLN10 + PCLN11 + PCLN12 + PCLN13 + PCLN14 
      Hyper =~ PCLN15 + PCLN16 + PCLN17 + PCLN18 + PCLN19 + PCLN20'

```

- First we compare higher and lower end of the distribution and iterate 500 times to get nice histogram

```{r}
conBigger10 <- iterateCFA(dataxCon, n_itr = 500, filter = '>', sample_size = 500) # failing
MASS::truehist(conBigger10$cfi, xlab = "CFI of all that higher than 10")
# now lower than ten

conSmaller10 <- iterateCFA(dataxCon, n_itr = 500, filter = '<', sample_size = 500)
MASS::truehist(conSmaller10$cfi, xlab = "CFI of all that lower than 10")
summary(conSmaller10)
summary(conBigger10)
```

- Plot presents CFI

### Sample size effects
- We test 100-1000 (steps of 50) possible sizes
- Begin with most frequent

```{r, echo=FALSE, warning=FALSE,  error=FALSE, message=FALSE}
# most frequent
# vecotr from sample size of 50 to 1000 in steps of 50
size_n <- seq(100,1000, 50)
res <- c()
for (i in 1:length(size_n)){
  print('sample size {size_n}')
  conBigger10 <- iterateCFA(dataxCon, n_itr = 30, filter = '>', sample_size = size_n[i]) # failing
  res[i] <- median(conBigger10$rmsea)
}

d <- data.frame(res,size_n)
ggplot(data=d, aes(y=res, x=size_n)) + geom_jitter() + geom_smooth() + ylab("rmsea") + xlab("Sample Size") + theme_minimal()
```

- Do the same with the lower tail of the distribution:
```{r, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
res1 <- c()
for (i in 1:length(size_n)){
  print('sample size {size_n}')
  conSmaller10 <- iterateCFA(dataxCon, n_itr = 30, filter = '<', sample_size = size_n[i]) # failing
  res1[i] <- median(conSmaller10$rmsea)
}

d1 <- data.frame(res1,size_n)
ggplot(data=d1, aes(y=res1, x=size_n)) + geom_jitter() + geom_smooth() +  ylab("rmsea") +xlab("Sample Size")+ theme_minimal()
```

- Lastly, all 

```{r, echo=FALSE, warning=FALSE,  message=FALSE, error=FALSE}
res2 <- c()
results_con <-  matrix(nrow = 30, ncol = 6)
for (i in 1:length(size_n)){
 # print('sample size {size_n}')
  for (n in 1:30){
    dfSamp <- sample_n(dfPCL5, size_n[i]) 
    pcl5Model <- cfa(dsm5Fac_Con, data = dfSamp[,1:20], estimator = "WLSMV")
    results_con[n,] <- fitMeasures(pcl5Model, c("chisq","df","pvalue","srmr","cfi","rmsea"))
    
  }
  
  res2[i] <- median(results_con[,6])
}

d2 <- data.frame(res2,size_n)
ggplot(data=d2, aes(y=res2, x=size_n)) + geom_jitter() + geom_smooth() +  xlab("Sample Size") + ylab("rmsea") + theme_minimal()
```

## Priniciple Component analysis -
- Without setting number of clusters

```{r}
# create correlation matrix
dataCor <- cor(dfPCL5)
fit <- princomp(dataCor, cor=TRUE)
summary(fit) # print variance accounted for
loadings(fit) # pc loadings
plot(fit,type="lines") # scree plot
fit$scores # the principal components
biplot(fit)
```

```{r}
# PCA Variable Factor Map
fit2 <- PCA(dfPCL5) # graphs generated automatically
get_eig(fit2)
# Visualize eigenvalues/variances
fviz_screeplot(fit2, addlabels = TRUE, ylim = c(0, 50))

```

```{r}
# Control variable colors using their contributions
fviz_pca_var(fit2, col.var="contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping
             )
```
```{r}
# Contributions of variables to PC1
fviz_contrib(fit2, choice = "var", axes = 1, top = 20)
# Contributions of variables to PC2
fviz_contrib(fit2, choice = "var", axes = 2, top = 20)
```
```{r}
km.res <- kmeans(scale(dfPCL5), 4, nstart = 25)
# 3. Visualize

fviz_cluster(km.res, data = dfPCL5,
             palette = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"),
             ggtheme = theme_minimal(),
             main = "Partitioning Clustering Plot"
             )
```
```{r}
res <- hcut(dfPCL5, k = 4, stand = TRUE)
res_d <- as.dendrogram(res)
plot(res_d)
library("ape")
plot(as.phylo(res), type = "unrooted", cex = 0.6,
     no.margin = TRUE)
# Visualize
fviz_dend(res_d, rect = TRUE, cex = 0.5,
          k_colors = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"))
```




