---
title: "Really 600k ways of PTSD"
author: "Or Duek & Tobias Spiller"
output:
  html_document:
    df_print: paged
---

## This work intends to adress 
- https://journals.sagepub.com/doi/10.1177/1745691613504115

- We will take a look at the DSM-5 data sets we have to see number of combination of symptoms of PTSD in DSM-V. 
- We could also look at the previous dataset of DSM-IV (150k patients) to see number of combinations there and compare the two
- Should take a look at somewhat similar work done in depression by Eiko Fried here: https://www.sciencedirect.com/science/article/pii/S0165032714006326?via%3Dihub


```{r}
## load libraries
require(foreign)
require(tidyverse)
require(bootnet)
require(qgraph)
library(BGGM)
library(GGMnonreg)
require(mgm)
require(EGAnet)
require(lavaan)
require(psychonetrics)
```

```{r}
# read dataset
df <- read.spss('/home/or/Documents/va_dsm5_data/PCL FY15-16.sav', to.data.frame = TRUE)
```

#### Organise the data a bit
- Number of NAs
- Plot histograms of pcl score and age

```{r organize}
# check nas
# calculate PCL5 scores A27-A46
df$totalPCL <- rowSums(df[21:40])
#hist(df$totalPCL)
sum(is.na(df$totalPCL))

df_clean <- filter(df, !is.na(df$totalPCL))
hist(df_clean$totalPCL)
hist(df_clean$Age)
```

#### Now we need to binaries the PCL symtpoms to 0 (no symptoms) or more than zero - for each pcl item

```{r}
# making sure the count procedure works as expected
id <- c(1,2,3,4)
a <-  c("a","b","a","b")
b <-  c("a","a","a","b")
c <-  c("b","b","b","a")

test <- data.frame(id,a,b,c)
test$id <- as.factor(test$id)
# count combinations
plyr::count(test[, -1])
```
### Create dataframe of PCL and id only - but with binary PCL scores
* Let us test with binary of pcl=>2 as yes and else is no (moderate symptoms)

```{r}
## PCL is a27 - a46 (PCL-5)
dfClean2 <- df_clean
for (i in 27:46){
  nam <- paste("A", i, sep = "")
  pcl <- paste("PCL", i-26, sep = "")
  dfClean2[pcl] <- dplyr::case_when(dfClean2[nam]<=1 ~ 'NO', dfClean2[nam]>=2 ~ 'YES')
 
}
```

### Check combinations

```{r}
# create specific dataframe
dfBinary2 <- data_frame(dfClean2$IDNUM, dfClean2[92:111])
countFreq2 <- plyr::count(dfBinary2[, -1])
```

### This creates 1650 combinations

```{r}
## count how many with only one
sum(countFreq2$freq==1)
# count more than one less than ten
sum(countFreq2$freq<10) - sum(countFreq2$freq==1)
max(countFreq2$freq)
2473 / nrow(dfBinary2)
```

1339 of the combination have only one individual. 
258 are less than 10 individuals but more than one.
2473 (32.7%) have same combination, which is the most prevalent in the dataset.

## Now we will do the same procedure on a second (smaller) DSM5 dataset

```{r}
df_new <- read.spss('/home/or/Documents/va_dsm5_data/psf19pcl.sav', to.data.frame = TRUE)
df_new$totalPCL <- rowSums(df_new[7:26])

df_newClean <- filter(df_new, !is.na(df_new$totalPCL))
hist(df_newClean$totalPCL)
summary(df_newClean$AGE_OCT01)
```
### We have 6656 valid observations. 
- Lets binarize using same method and count combinations

```{r}
df_newClean2 <- df_newClean
for (i in 1:20){
  # becuase under 10 its PCLN01 (02,03) need to add zero so run if
  if (i<10){
  nam <- paste("PCLN0", i, sep = "")  
  }
  else {
    nam <- paste("PCLN", i, sep = "")
  }
  pcl <- paste("PCL", i, sep = "")
  df_newClean2[pcl] <- dplyr::case_when(df_newClean2[nam]<=1 ~ 'NO', df_newClean2[nam]>=2 ~ 'YES')
  
}
```

```{r}
# create specific dataframe
dfNew_Binary2 <- data_frame(df_newClean2$ID, df_newClean2[63:82])
countFreq_new2 <- plyr::count(dfNew_Binary2[, -1])
```

### Total of 1153 combinations. Lets look into them more carefully

```{r}
## count how many with only one
sum(countFreq_new2$freq==1)
# count more than one less than ten
sum(countFreq_new2$freq<10) - sum(countFreq_new2$freq==1)
max(countFreq_new2$freq)
2858 / nrow(dfNew_Binary2)
```

### Some data on the combinations
- 904 of the 1153 have only one individual. 
- 208 have more than one but less than 10 individuals
- 2858 individuals are in the most prevalent combination (43%)


### Lets add DSM-IV to the analysis (using data from previous study published in ...)

```{r}
# load data set
source('/home/or/Documents/va_data/readData.r')
```

```{r}
# all patientes with PTSD and PCLTOT
pclAll <- dplyr::filter(vaDatclean, !is.na(BPCLTOT))
# plot pcl total score 
hist(pclAll$BPCLTOT)
# we have a minimum of 2 - so we have some NAs - let remove them
pclAll_Nas <- filter(pclAll, BPCLTOT <=16)
# total of 20 subjects with 16 or less in PCL (i.e. at least one missing variable)
# we can remove them from analysis
pclAll <- filter(pclAll, BPCLTOT >=17)
# 159577 patients
#pclNetwork <- pclNoNa # just medicated
pclNetwork <- pclAll
nrow(pclNetwork)
hist(pclNetwork$BPCLTOT)
```

```{r}
# take just pcl
pclItems <- dplyr::select(pclAll, starts_with("PCL"))
pclItems_noCluster <- dplyr::select(pclItems, -PCLFY, -PCLSURVEYDATE, -PCLRAWSCORE)
nrow(pclItems_noCluster)
pclItems_noCluster <- na.omit(pclItems_noCluster)
nrow(pclItems_noCluster)
```

Total of 158,139 patients with valid PCL-4 scores (PCL-M)
Now we can run the analysis similar to before (binarize and count combinations)

```{r}
dfPCL4 <- pclItems_noCluster
# using PCL-4 recommended threshold is 3+ (not 2+ as in PCL5)
for (i in 1:17){
  nam = paste("PCL", i, sep = "")
  pcl <- paste("PCLB", i, sep = "")
  dfPCL4[pcl] <- dplyr::case_when(dfPCL4[nam]<=2 ~ 'NO', dfPCL4[nam]>=3 ~ 'YES')
  
}
```

```{r}
# create specific dataframe
id = 1:nrow(dfPCL4)
dfPCL4_Binary <- data_frame(id, dfPCL4[18:34])
countFreq_pcl4 <- plyr::count(dfPCL4_Binary[,-1])
```

### OK - so number of combinations is 25,253 total
- Lets look at the results a bit

```{r}
## count how many with only one
sum(countFreq_pcl4$freq==1)
# percentage
sum(countFreq_pcl4$freq==1) / nrow(countFreq_pcl4)
# count more than one less than ten
sum(countFreq_pcl4$freq<10) - sum(countFreq_pcl4$freq==1)
max(countFreq_pcl4$freq)
max(countFreq_pcl4$freq) / nrow(dfPCL4_Binary)
```

### Sum results
- 15,479 combinations are with only 1 individual (59% of combinations)
- 9368 are less than 10 individuals but more than one
- 28,901 individuals are present in the most frequent combination of symptoms (18% of patients)


### Last thing - lets randomly sample 7k people, to see how it affects the number of combinations

```{r}
# sample
#pcl4Sample <- sample(dfPCL4_Binary, size = 7000)
picked = sample(seq_len(nrow(dfPCL4_Binary)),size = 7000)
pcl4Sample <- dfPCL4_Binary[picked,]
countFreq_pcl4Samples <- plyr::count(pcl4Sample[,-1])
```

### Results of sampling:

Now we have only 2813 combinations. So the size of dataset definetly affects number of possible combinations. BUT - it is also important to state that while DSM-5 has app. 600k combinations, DSM-IV has around 70K and even with a large sample size of 150k patients, we have found a maximum of 28k combinations, which is less than half of possible combinations. 


### Last play with data - loop through randomization and print how many combination we get with 7k patients.
```{r}
# sample
#pcl4Sample <- sample(dfPCL4_Binary, size = 7000)
x <- 1:1000
for (i in 1:1000){
  picked = sample(seq_len(nrow(dfPCL4_Binary)),size = 7000)
  pcl4Sample <- dfPCL4_Binary[picked,]
  countFreq_pcl4Samples <- plyr::count(pcl4Sample[,-1])
  x[i] <- nrow(countFreq_pcl4Samples)
}

MASS::truehist(x)
```

### Summary

- DSM-IV with large dataset (150k) shows a 26,253 combinations
- DSM-IV with randomly sampled smaller sets (7k) shows around 2800 combinations
- DSM-V shows 1650 (sample 1) and 1153 (sample 2) combinations
- Interestingly, DSM-IV shows more actual combination of symptoms, even when accounting for sample size


* so - sample size indeed affects the number of actual combinations, but even when accounting for sample size we still see much lower numbers compared to the theoretically possible 600k combinations of DSM-V.


## Repeating Glazer's calculation in real life

- Here we will conduct a similar calculation to the one presented in the 600k ways paper.
- Mainly $\rightarrow$ calculating each cluster according to DSM 4/5

### Starting with DSM-V data

- PCL5:
* B (Intrusion): Items 1-5
* C (Avoidance): Items 6-7
* D () : Items 8-14
* E () : Items 15-20

#### Start with large dataset:

```{r}
dfBinary2 <- data_frame(dfClean2$IDNUM, dfClean2[92:111])
# intrusion
dfBinary2_int <- dfBinary2[2:6]
countFreq2_int <- plyr::count(dfBinary2_int) # 31 combinations (like Galazer et al.)
# avoidance
dfBinary2_avoid <- dfBinary2[7:8]
countFreq2_avoid <- plyr::count(dfBinary2_avoid) # 4 combinations (one is no no)
# mood cog
dfBinary2_mood <- dfBinary2[9:15]
countFreq2_mood <- plyr::count(dfBinary2_mood) # 120 combinations 

# hypervigilance
dfBinary2_hyper <- dfBinary2[16:21]
countFreq2_hyper <- plyr::count(dfBinary2_hyper) # 61 combinations

31 * 3 * 120 * 61


```
### So here we basically get even more the Galazer anticipated, just because we also have some that marked NO on any symptom of a cluster. 

## Now to Tobias's suggestions:
1. Power law - plot frequencies

```{r}
# we have three frequency tables:
#DSMV big = countFreq2
#DSMV small = countFreq_new2
# DSM-IV = countFreq_pcl4
# DSM-IV random sample = countFreq_pcl4Samples

# order by freq
# plot
freq1 <- countFreq2[order(countFreq2$freq, decreasing = TRUE),]
ggplot(freq1, aes(x=as.factor(1:nrow(freq1)),y=freq)) + geom_bar(stat="identity")
```

### This is hard to follow, as we have many combinations with only one individual. 
- lets take the top 50

```{r}
freq1_top50 <- freq1[1:50,]
x <- as.factor(1:nrow(freq1_top50))
ggplot(freq1_top50, aes(x=as.factor(x),y=freq)) + geom_bar(stat="identity") + xlab("Combination Of Symptoms") + theme_minimal()
  
```

### Now for the second DSM-V dataset

```{r}
freq2 <- countFreq_new2[order(countFreq_new2$freq, decreasing = TRUE),]
freq2_top50 <- freq2[1:50,]
x <- as.factor(1:nrow(freq2_top50))
ggplot(freq2_top50, aes(x=as.factor(x),y=freq)) + geom_bar(stat="identity") + xlab("Combination Of Symptoms") + theme_minimal()
```
### Now to DSM-IV large dataset

```{r}
freq3 <- countFreq_pcl4[order(countFreq_pcl4$freq, decreasing = TRUE),]
freq3_top50 <- freq3[1:50,]
x <- as.factor(1:nrow(freq3_top50))
ggplot(freq3_top50, aes(x=as.factor(x),y=freq)) + geom_bar(stat="identity") + xlab("Combination Of Symptoms") + theme_minimal()

```

### Lastly for a randomly samples 7k from the large DSM-IV dataset

```{r}
freq4 <- countFreq_pcl4Samples[order(countFreq_pcl4Samples$freq, decreasing = TRUE),]
freq4_top50 <- freq4[1:50,]
x <- as.factor(1:nrow(freq4_top50))
ggplot(freq4_top50, aes(x=as.factor(x),y=freq)) + geom_bar(stat="identity") + xlab("Combination Of Symptoms") + theme_minimal()
```
### Pareto Q-Q plots

```{r}
require(ReIns)
ParetoQQ(data = freq1$freq, main = "DSM-V 1st Sample")
```

```{r}
ParetoQQ(data = freq2$freq, main = "DSM-V 2nd Sample")
```
```{r}
ParetoQQ(data = freq3$freq, main = "DSM-IV large Sample")
```
```{r}
ParetoQQ(data = freq4$freq, main = "DSM-IV random 7k Sample")
```


## Next - Tobias offered portfolio of most endorsed profiles. 
* For now we will just present the first one from each dataset

```{r}
print(freq1[1,]) # this one endorses all symptoms :(
print(freq2[1,]) # again - all symptoms
# DSM-IV large sample
print(freq3[1,]) # same here - all symptoms
print(freq4[1,]) # also all symptoms
```

### Ok this is interesting, but because its all very similar, lets take the second most endorsed profile in each

```{r}
print(freq1[2,]) # this one endorses all symptoms except PCL 8 (amnesia). Which was expected 
print(freq2[2,]) # all symptoms, but not endorsing PCL16 (taking too many risks or doing things that can cause harm)
# DSM-IV large sample
print(freq3[2,]) # all symptoms but PCL8 (amnesia)
print(freq4[2,]) # same as abova
```


## OK, last comment was regarding distance between vectors of symptoms

```{r}
## calculate distance between first and second vector to test the idea
d <- 20 - sum(freq1[1,1:20]== freq1[2,1:20])
# now lets for loop across all with first (most prevalent) as reference
n <- 20 # set number of symptoms
dist1 = 1:nrow(freq1) # vector for distance
sympNum1 <- 1:nrow(freq1) # vector for number of endorsed symptoms
for (i in 1:nrow(freq1)) {
  d <- n - sum(freq1[1,1:20] == freq1[i,1:20])
  dist1[i] = d
  sympNum1[i] <- sum(freq1[i,1:20]=="YES")
}
# will plot symptoms and distance 
plot(dist1, sympNum1)
```

* We should note that reference for all samples is basically endorsing everything, so we will probably have this straight line anyhow, longer distance = less symptoms.

### Lets run same analysis on the other samples

- DSM-5 2nd sample

```{r}
## calculate distance between first and second vector to test the idea

# now lets for loop across all with first (most prevalent) as reference
n <- 20 # set number of symptoms
dist2 = 1:nrow(freq2) # vector for distance
sympNum2 <- 1:nrow(freq2) # vector for number of endorsed symptoms
for (i in 1:nrow(freq2)) {
  d <- n - sum(freq2[1,1:20] == freq2[i,1:20])
  dist2[i] = d
  sympNum2[i] <- sum(freq2[i,1:20]=="YES")
}
# will plot symptoms and distance 
plot(dist2, sympNum2)
hist(dist2)
```

### Now DSM-IV
- Large sample
```{r}
n <- 17 # set number of symptoms
dist3 = 1:nrow(freq3) # vector for distance
sympNum3 <- 1:nrow(freq3) # vector for number of endorsed symptoms
for (i in 1:nrow(freq3)) {
  d <- n - sum(freq3[1,1:17] == freq3[i,1:17])
  dist3[i] = d
  sympNum3[i] <- sum(freq3[i,1:17]=="YES")
  }
# will plot symptoms and distance 
plot(dist3, sympNum3)
hist(dist3)
```


### OK - picture is clear. 

## Summary:
* We can see much less possible combinations than expected.
* It seems like frequency of each symptoms' combination is close to compatible with power law
* The most frequent profile endores (by far) is all symptoms
* The second most is usually all syptoms but amnesia (with one sample shows all symptoms but risky behaviour)

